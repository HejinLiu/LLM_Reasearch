{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_bib_info(bib_filename, conf_name, year):\n",
    "    \"\"\"\n",
    "    从BibTeX文件中提取信息，并将其保存到Excel文件中。\n",
    "\n",
    "    Args:\n",
    "    bib_filename (str): BibTeX文件名。\n",
    "    conf_name (str): 会议名称。\n",
    "    year (int): 会议年份。\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: 包含提取信息的DataFrame。\n",
    "    \"\"\"\n",
    "    # 读取BibTeX文件\n",
    "    with open(bib_filename, 'r', encoding='utf-8') as bibfile:\n",
    "        bib_database = bibtexparser.load(bibfile)\n",
    "\n",
    "    # 创建一个空的DataFrame来存储提取的信息\n",
    "    data = {'title': [],\n",
    "            'author': [],\n",
    "            'year': [],\n",
    "            'month': [],\n",
    "            'address': [],\n",
    "            'publisher': [],\n",
    "            'url': [],\n",
    "            'doi': [],\n",
    "            'pages': []}\n",
    "\n",
    "    # 遍历每个条目，并提取所需信息\n",
    "    for entry in bib_database.entries:\n",
    "        data['title'].append(entry.get('title', ''))\n",
    "        data['author'].append(entry.get('author', ''))\n",
    "        data['year'].append(entry.get('year', ''))\n",
    "        data['month'].append(entry.get('month', ''))\n",
    "        data['address'].append(entry.get('address', ''))\n",
    "        data['publisher'].append(entry.get('publisher', ''))\n",
    "        data['url'].append(entry.get('url', ''))\n",
    "        data['doi'].append(entry.get('doi', ''))\n",
    "        data['pages'].append(entry.get('pages', ''))\n",
    "\n",
    "    # 将提取的信息转换为DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 将DataFrame写入Excel文件\n",
    "    excel_filename = f\"{conf_name}{year}.xlsx\"\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "\n",
    "    print(f\"Excel文件 {excel_filename} 已创建成功！\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_html_info(html_filename, conf_name, year):\n",
    "    \"\"\"\n",
    "    从HTML文件中提取文章标题和摘要，并将其添加到Excel文件中。\n",
    "\n",
    "    Args:\n",
    "    html_filename (str): HTML文件名。\n",
    "    conf_name (str): 会议名称。\n",
    "    year (int): 会议年份。\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: 包含提取信息的DataFrame。\n",
    "    \"\"\"\n",
    "    # 读取HTML文件\n",
    "    with open(html_filename, 'r', encoding='utf-8') as html_file:\n",
    "        html_content = html_file.read()\n",
    "\n",
    "    # 使用Beautiful Soup解析HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # 创建一个空的DataFrame来存储文章信息\n",
    "    data = {'paper_title': [], 'abstract': []}\n",
    "    # 填充两行空白数据\n",
    "    data['paper_title'].append('')\n",
    "    data['abstract'].append('')\n",
    "    if year == 2023 and conference == \"ACL\":\n",
    "        data['paper_title'].append('')\n",
    "        data['abstract'].append('')\n",
    "\n",
    "    # 设置i的初始值\n",
    "    i = 1\n",
    "\n",
    "    # 循环搜索文章标题和摘要内容并提取\n",
    "    while True:\n",
    "        # 构建文章链接的正则表达式\n",
    "\n",
    "        str_acl = f'https://aclanthology.org/{year}.acl-long.{i}/'\n",
    "        str_emnlp = f'https://aclanthology.org/{year}.findings-emnlp.{i}/'\n",
    "\n",
    "        if conference == \"ACL\":\n",
    "            paper_link_regex = re.compile(fr'href={str_acl}')\n",
    "        elif conference == \"EMNLP\":\n",
    "            paper_link_regex = re.compile(fr'href={str_emnlp}')\n",
    "        # paper_link_regex = re.compile(fr'href=\"https://aclanthology.org/{year}.acl-long.{i}/\"')\n",
    "\n",
    "        # 搜索文章链接\n",
    "        paper_link_match = re.search(paper_link_regex, html_content)\n",
    "\n",
    "        # 如果找到文章链接\n",
    "        if paper_link_match:\n",
    "            # 根据文章链接定位文章标题\n",
    "            if conference == \"ACL\":\n",
    "                paper_link_tag = soup.find(href=str_acl)\n",
    "            elif conference == \"EMNLP\":\n",
    "                paper_link_tag = soup.find(href=str_emnlp)\n",
    "\n",
    "            if paper_link_tag:\n",
    "                paper_title = paper_link_tag.text.strip()\n",
    "                data['paper_title'].append(paper_title)\n",
    "                print(f\"文章 {i} 标题提取成功: {paper_title[:20]}...\")\n",
    "            else:\n",
    "                data['paper_title'].append('')\n",
    "                print(f\"文章 {i} 标题未找到\")\n",
    "\n",
    "            # 根据摘要id定位摘要内容\n",
    "            if conference == \"ACL\":\n",
    "                abstract_div = soup.find(id=f'abstract-{year}--acl-long--{i}').find_next('div')\n",
    "            elif conference == \"EMNLP\":\n",
    "                abstract_div = soup.find(id=f'abstract-{year}--findings-emnlp--{i}').find_next('div')\n",
    "\n",
    "            # 提取摘要文本并添加到DataFrame\n",
    "            if abstract_div:\n",
    "                abstract_text = abstract_div.text.strip()\n",
    "                data['abstract'].append(abstract_text)\n",
    "                print(f\"文章 {i} 摘要提取成功: {abstract_text[:20]}...\")\n",
    "            else:\n",
    "                data['abstract'].append('')\n",
    "                print(f\"摘要 {i} 未找到\")\n",
    "\n",
    "            # 增加i的值\n",
    "            i += 1\n",
    "        else:\n",
    "            # 如果找不到文章链接，退出循环\n",
    "            break\n",
    "\n",
    "    # 将提取的文章信息转换为DataFrame\n",
    "    df_articles = pd.DataFrame(data)\n",
    "\n",
    "    # 读取原始Excel文件\n",
    "    excel_filename = f\"{conf_name}{year}.xlsx\"\n",
    "    df_original = pd.read_excel(excel_filename)\n",
    "\n",
    "    # 将文章标题信息添加为原始Excel文件的新列\n",
    "    df_original['paper_title'] = df_articles['paper_title']\n",
    "    df_original['abstract'] = df_articles['abstract']\n",
    "\n",
    "    # 将更新后的DataFrame写回原始Excel文件\n",
    "    df_original.to_excel(excel_filename, index=False)\n",
    "\n",
    "    print(f\"文章标题和摘要信息已提取并添加到原始Excel文件 {excel_filename} 中！\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fill_title_with_paper_title(conf_name, year):\n",
    "    \"\"\"\n",
    "    从Excel文件中读取数据，将paper_title填充到title列中，并将更新后的DataFrame写回Excel文件中。\n",
    "\n",
    "    Args:\n",
    "    conf_name (str): 会议名称。\n",
    "    year (int): 会议年份。\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # 读取Excel文件\n",
    "    excel_filename = f\"{conf_name}{year}.xlsx\"\n",
    "    df = pd.read_excel(excel_filename)\n",
    "\n",
    "    # 遍历每一行\n",
    "    for index, row in df.iterrows():\n",
    "        # 获取当前行的paper_title和title属性值\n",
    "        paper_title = row['paper_title']\n",
    "\n",
    "        # 如果paper_title为空，则跳过该行\n",
    "        if pd.isna(paper_title):\n",
    "            print(f\"行 {index + 1}: paper_title为空，跳过处理\")\n",
    "            continue\n",
    "\n",
    "        # 如果paper_title不为空，则用其内容填充title属性\n",
    "        df.at[index, 'title'] = paper_title\n",
    "        print(f\"行 {index + 1}: paper_title为 '{paper_title[:10]}'... 用其填充title属性\")\n",
    "\n",
    "    # 将更新后的DataFrame写回Excel文件\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "\n",
    "    print(f\"Excel文件 {excel_filename} 处理完成！\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conference = \"EMNLP\"\n",
    "year = 2023\n",
    "\n",
    "# 提取BibTeX信息并保存到Excel文件中\n",
    "extracted_df = extract_bib_info(bib_filename=f'{conference}{year}.bib', conf_name=conference, year=year)\n",
    "\n",
    "# 提取HTML信息并添加到对应的Excel文件中\n",
    "extract_html_info(f'{conference}{year}.html', conf_name=conference, year=year)\n",
    "\n",
    "# 将2023年的paper_title填充到title列中\n",
    "fill_title_with_paper_title(conf_name=conference, year=year)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top three words in titles for 2019: ['neural', 'translation', 'machine']\n",
      "Top three words in titles for 2020: ['neural', 'generation', 'translation']\n",
      "Top three words in titles for 2021: ['neural', 'translation', 'generation']\n",
      "Top three words in abstracts for 2021: ['knowledge', 'state-of-the-art', 'information']\n",
      "Top three words in titles for 2022: ['generation', 'neural', 'translation']\n",
      "Top three words in abstracts for 2022: ['knowledge', 'state-of-the-art', 'information']\n",
      "Top three words in titles for 2023: ['generation', 'knowledge', 'extraction']\n",
      "Top three words in abstracts for 2023: ['knowledge', 'generation', 'novel']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 创建保存图片的文件夹\n",
    "if not os.path.exists('figure_ACL21-23'):\n",
    "    os.makedirs('figure_ACL21-23')\n",
    "\n",
    "# 读取Excel文件\n",
    "acl2019_df = pd.read_excel('ACL2019.xlsx')\n",
    "acl2020_df = pd.read_excel('ACL2020.xlsx')\n",
    "acl2021_df = pd.read_excel('ACL2021.xlsx')\n",
    "acl2022_df = pd.read_excel('ACL2022.xlsx')\n",
    "acl2023_df = pd.read_excel('ACL2023.xlsx')\n",
    "\n",
    "# 将NaN值替换为空字符串\n",
    "for df in [acl2019_df, acl2020_df, acl2021_df, acl2022_df, acl2023_df]:\n",
    "    df.replace(np.nan, '', inplace=True)\n",
    "\n",
    "# 获取NLTK停用词列表\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# 自定义常见词，除了NLTK停用词之外\n",
    "common_words = {'model', 'propose', 'based', 'via', 'task', 'dataset', 'paper', 'models', 'method', 'text', 'datasets', 'data', 'tasks', 'performance', 'training', 'language', 'methods', 'using', 'new', 'show', 'However', 'also', 'two','results','existing','learning','different','however,','paper,','experiments','demonstrate'}\n",
    "\n",
    "# 将NLTK停用词与自定义常见词合并\n",
    "common_words = common_words.union(nltk_stopwords)\n",
    "\n",
    "# 定义函数用于生成词频统计和词云图\n",
    "def clean_title(title):\n",
    "    # 移除无效字符，并转换为小写\n",
    "    cleaned_title = ' '.join(word.strip().lower() for word in title.split() if word.strip() and word.strip().lower() not in common_words)\n",
    "    return cleaned_title\n",
    "\n",
    "def plot_word_frequency_and_cloud(dataframe, year):\n",
    "    # 对title进行词频统计和清洗\n",
    "    dataframe['cleaned_title'] = dataframe['title'].apply(clean_title)\n",
    "    title_words = dataframe['cleaned_title'].str.split().explode()\n",
    "    title_words = title_words[title_words != '']  # 去除空字符串\n",
    "    title_word_counts = title_words.value_counts()\n",
    "    title_word_counts_top30 = title_word_counts.head(30)  # 只取前30个词\n",
    "    top_three_words_title = title_word_counts_top30.head(3).index.tolist()  # 获取标题前三个词\n",
    "    print(f\"Top three words in titles for {year}: {top_three_words_title}\")  # 输出标题前三个词\n",
    "\n",
    "    # 绘制标题词频统计图并保存到本地文件\n",
    "    plt.figure(figsize=(12, 8))  # 增加图形大小\n",
    "    title_word_counts_top30.plot(kind='barh', color='skyblue')\n",
    "    plt.title(f'Title Word Frequency {year}')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figure_ACL21-23/title_word_frequency_{year}.png')  # 保存到本地文件夹\n",
    "    plt.close()  # 关闭绘图，释放资源\n",
    "\n",
    "    # 生成标题词云图并保存到本地文件\n",
    "    title_wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=common_words).generate(' '.join(title_words))\n",
    "    plt.figure(figsize=(12, 8))  # 增加图形大小\n",
    "    plt.imshow(title_wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Title Word Cloud {year}')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figure_ACL21-23/title_word_cloud_{year}.png')  # 保存到本地文件夹\n",
    "    plt.close()  # 关闭绘图，释放资源\n",
    "\n",
    "    # 如果是ACL2021、2022、2023年，对摘要进行词频统计和绘制词云图\n",
    "    if year in [2021, 2022, 2023]:\n",
    "        # 对abstract进行词频统计和清洗\n",
    "        dataframe['cleaned_abstract'] = dataframe['abstract'].apply(clean_title)\n",
    "        abstract_words = dataframe['cleaned_abstract'].astype(str).str.split().explode()\n",
    "        abstract_words = abstract_words[abstract_words != '']  # 去除空字符串\n",
    "        abstract_word_counts = abstract_words.value_counts()\n",
    "\n",
    "        # 绘制摘要词频统计图并保存到本地文件\n",
    "        abstract_word_counts_top30 = abstract_word_counts.head(30)  # 只取前30个词\n",
    "        top_three_words_abstract = abstract_word_counts_top30.head(3).index.tolist()  # 获取摘要前三个词\n",
    "        print(f\"Top three words in abstracts for {year}: {top_three_words_abstract}\")  # 输出摘要前三个词\n",
    "        plt.figure(figsize=(12, 8))  # 增加图形大小\n",
    "        abstract_word_counts_top30.plot(kind='barh', color='lightgreen')\n",
    "        plt.title(f'Abstract Word Frequency {year}')\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Words')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figure_ACL21-23/abstract_word_frequency_{year}.png')  # 保存到本地文件夹\n",
    "        plt.close()  # 关闭绘图，释放资源\n",
    "\n",
    "        # 生成摘要词云图并保存到本地文件\n",
    "        abstract_wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=common_words).generate(' '.join(abstract_words))\n",
    "        plt.figure(figsize=(12, 8))  # 增加图形大小\n",
    "        plt.imshow(abstract_wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Abstract Word Cloud {year}')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'figure_ACL21-23/abstract_word_cloud_{year}.png')  # 保存到本地文件夹\n",
    "        plt.close()  # 关闭绘图，释放资源\n",
    "\n",
    "\n",
    "\n",
    "# 对ACL2019和ACL2020年调用函数生成标题的统计图和词云图\n",
    "plot_word_frequency_and_cloud(acl2019_df, 2019)\n",
    "plot_word_frequency_and_cloud(acl2020_df, 2020)\n",
    "\n",
    "# 对ACL2021、2022、2023年调用函数生成标题和摘要的统计图和词云图\n",
    "plot_word_frequency_and_cloud(acl2021_df, 2021)\n",
    "plot_word_frequency_and_cloud(acl2022_df, 2022)\n",
    "plot_word_frequency_and_cloud(acl2023_df, 2023)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
